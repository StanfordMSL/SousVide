<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Project: [Project Title]</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.1.0/github-markdown.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f9f9f9;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background: #fff;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        .header {
            text-align: center;
            margin-bottom: 40px;
        }
        .header h1 {
            font-size: 2.5em;
        }
        .header p {
            font-size: 1.2em;
            color: #555;
        }
        .section {
            margin-bottom: 30px;
        }
        .section h2 {
            font-size: 1.8em;
            color: #444;
            margin-bottom: 10px;
        }
        .section p {
            margin-bottom: 15px;
        }
        .images-row {
            display: flex;
            justify-content: space-between;
            gap: 10px;
        }
        .images-row img {
            width: 48%;
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .video-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            height: 0;
            overflow: hidden;
            max-width: 100%;
            background: #000;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .code-block {
            background: #f4f4f4;
            border-left: 5px solid #ccc;
            padding: 10px;
            font-family: Consolas, monaco, monospace;
        }
        .footer {
            text-align: center;
            margin-top: 50px;
            font-size: 0.9em;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="container markdown-body">
        <div class="header">
            <h1>SOUS VIDE</h1>
            <p>Scene Optimized Understanding via Synthesized Visual Inertial Data from Experts</p>
        </div>
        <img src="assets/images/PipelineDiagram_v3.png" alt="Sous Vide Pipeline" style="max-width: 100%; border: 1px solid #ddd;">
        <!-- <img src="assets/images/FirstFlights.png" alt="First Flights" style="max-width: 100%; border: 1px solid #ddd;">

        <div class="images-row">
            <img src="assets/images/Samples_v3.png" alt="Sous Vide Pipeline">
            <img src="assets/images/Novel.png" alt="Result 2">
        </div> -->
        <div class="section">
            <h2>Demonstration</h2>
            <p>Main Videos:</p>
            <div class="video-container">
                <iframe src="https://youtu.be/IhZeXXJ47Js" frameborder="0" allowfullscreen></iframe>
            </div>
            <div class="video-container" style="margin-top: 20px;">
                <iframe src="https://youtu.be/NKAVm2tnyTc" frameborder="0" allowfullscreen></iframe>
            </div>
        </div>

        <div class="section">
            <h2>Abstract</h2>
            <p>We propose a new simulator, training approach, and
            policy architecture, collectively called SOUS VIDE, for end-to-
            end visual drone navigation. Our trained policies exhibit zero-
            shot sim-to-real transfer with robust real-world performance
            using only on-board perception and computation. Our simulator,
            called FiGS, couples a computationally simple drone dynamics
            model with a high visual fidelity Gaussian Splatting scene re-
            construction. FiGS can quickly simulate drone flights producing
            photo-realistic images at over 100 fps. We use FiGS to collect
            100k-300k observation-action pairs from an expert MPC with
            privileged state and dynamics information, randomized over
            dynamics parameters and spatial disturbances. We then distill
            this expert MPC into an end-to-end visuomotor policy with a
            lightweight neural architecture, called SV-Net. SV-Net processes
            color image and IMU data streams into low-level body rate and
            thrust commands at 20Hz onboard a drone. Crucially, SV-Net
            includes a Rapid Motor Adaptation (RMA) module that adapts
            at runtime to variations in the dynamics parameters of the drone.
            In extensive hardware experiments, we show SOUS VIDE polices
            to be robust to ±30% mass and thrust variations, 40 m/s wind
            gusts, 60% changes in ambient brightness, shifting or removing
            objects from the scene, and people moving aggressively through
            the drone’s visual field. The project page and code can be found</p>
        </div>

        <div class="section">
            <h2>Key Contributions</h2>
            <ul>
                <li><strong>Flying in Gaussian Splats (FiGS):</strong>A simulator that couples a GSplat scene model with a lightweight drone dynamics model to yield photorealistic visual flight data.</li>
                <li><strong>Expert Demonstration Data:</strong>We use an MPC expert to generate behavior cloning data in FiGS with randomized dynamics parameters and positional disturbances.</li>
                <li><strong>SV-Net:</strong>We introduce a lightweight policy architecture that takes image and IMU data to infer thrust and body rate control actions.  The policy uses an RMA module to adapt online to varying flight conditions.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Getting Started</h2>
            <p>The codebase accompanying this research is available on this repository. To replicate our experiments, follow these steps:</p>
            <ol>
                <li>Clone the repository:</li>
                <div class="code-block">
                    <code>git clone https://github.com/username/repository-name.git</code>
                </div>
                <li>Install dependencies:</li>
                <div class="code-block">
                    <code>pip install -r requirements.txt</code>
                </div>
                <li>Run the experiments:</li>
                <div class="code-block">
                    <code>python run_experiments.py</code>
                </div>
            </ol>
        </div>

        <div class="section">
            <h2>Results</h2>
            <p>This work introduces SOUS VIDE, a novel training
                paradigm leveraging Gaussian Splatting and lightweight vi-
                suomotor policy architectures for end-to-end drone navigation.
                By coupling high-fidelity visual data synthesis with online
                adaptation mechanisms, SOUS VIDE achieves zero-shot sim-
                to-real transfer, demonstrating remarkable robustness to varia-
                tions in mass, thrust, lighting, and dynamic scene changes. Our
                experiments underscore the policy’s ability to generalize across
                diverse scenarios, including complex and extended trajectories,
                with graceful degradation under extreme conditions. Notably,
                the integration of a streamlined adaptation module enabled the
                policy to overcome limitations of prior visuomotor approaches,
                offering a computationally efficient yet effective solution for
                addressing model inaccuracies.
                These findings highlight the potential of SOUS VIDE as
                a foundation for future advancements in autonomous drone
                navigation. While its robustness and versatility are evident,
                challenges such as inconsistent performance in multi-objective
                tasks suggest opportunities for improvement through more
                sophisticated objective encodings. Further exploration into
                scaling the approach to more complex environments and in-
                corporating additional sensory modalities could enhance both
                adaptability and reliability. Ultimately, this work paves the
                way for deploying learned visuomotor policies in real-world
                applications, bridging the gap between simulation and practical
                autonomy in drone operations.</p>
        </div>

        <div class="section">
            <h2>Publication</h2>
            <p>This research is detailed in our paper titled <em>"[Paper Title]"</em>, published at [Conference/Journal Name].</p>
            <p><a href="[link-to-paper]" target="_blank">Read the full paper here</a>.</p>
        </div>

        <div class="section">
            <h2>Acknowledgments</h2>
            <p>This work was supported in part by DARPA grant HR001120C0107, ONR grant N00014-23-1-2354, and Lincoln Labs grant 7000603941. The second author was supported on an NDSEG fellowhsip. Toyota Research Institute provided funds to support this work.</p>
        </div>

        <div class="section">
            <h2>License</h2>
            <p>This project is licensed under the GNU General Public License v3.0. See the <a href="LICENSE">LICENSE</a> file for more details.</p>
        </div>

        <div class="footer">
            <p>&copy; 2024 [Your Name/Organization]. All rights reserved.</p>
        </div>
    </div>
</body>
</html>
